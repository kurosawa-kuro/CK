# CKA特化: クラスタアーキテクチャとコンポーネント

### 実技問題との対応

| 実技問題 | 使用する知識 |
|---------|------------|
| 問題4: DaemonSet | コントロールプレーンのTaint理解、kube-proxy/kindnetの動作 |
| 問題8: DNS | CoreDNS、Service FQDN、Pod内DNS設定 |

## コントロールプレーン構成

```
┌─────────────────────────────────────────────────────────────┐
│                    Control Plane Node                        │
│  ┌─────────────┐  ┌──────────────────┐  ┌───────────────┐  │
│  │ kube-apiserver│  │kube-controller-  │  │kube-scheduler │  │
│  │              │  │    manager       │  │               │  │
│  └──────┬───────┘  └────────┬─────────┘  └───────┬───────┘  │
│         │                   │                     │          │
│         └───────────────────┼─────────────────────┘          │
│                             │                                │
│                       ┌─────▼─────┐                         │
│                       │   etcd    │                         │
│                       └───────────┘                         │
└─────────────────────────────────────────────────────────────┘
```

---

## 1. kube-apiserver

### 役割
- クラスタへの**唯一のエントリポイント**
- RESTful APIを提供
- 認証・認可・Admission Controlを実行
- etcdへの読み書きを担当

### 試験ポイント
```bash
# API Serverの確認
kubectl get pods -n kube-system | grep apiserver

# 静的Podのマニフェスト場所
/etc/kubernetes/manifests/kube-apiserver.yaml

# API Serverが停止すると
- kubectl コマンドが使えなくなる
- 新規リソース作成不可
- ただし既存Podは動き続ける（重要！）
```

### 主要フラグ（試験で見る）
| フラグ | 説明 |
|--------|------|
| `--etcd-servers` | etcdのエンドポイント |
| `--service-cluster-ip-range` | Service用のIP範囲 |
| `--client-ca-file` | クライアント証明書のCA |
| `--tls-cert-file` | API Serverの証明書 |

---

## 2. etcd

### 役割
- 分散Key-Valueストア
- クラスタの**全ての状態**を保存
- 唯一の永続的データストア

### 試験ポイント
```bash
# etcdの場所
/etc/kubernetes/manifests/etcd.yaml

# etcdに保存されるもの
- Pod, Deployment, Service等の定義
- ConfigMap, Secret
- RBAC設定
- Namespace

# etcdに保存されないもの（重要！）
- Node のリアルタイムメトリクス（CPU使用率等）
- コンテナログ
- ノード上の実行状態
```

### バックアップコマンド（必須暗記）
```bash
ETCDCTL_API=3 etcdctl snapshot save /backup/etcd-snapshot.db \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key
```

---

## 3. kube-scheduler

### 役割
- Podをどのノードに配置するか決定
- **配置のみ**（起動はkubeletの仕事）

### スケジューリング判断基準
```yaml
# スケジューラが見るもの
- requests.cpu / requests.memory  # リソース要求
- nodeSelector                     # ノード選択
- nodeName                         # 直接指定
- taints/tolerations              # 排除条件
- affinity/anti-affinity          # 親和性

# スケジューラが見ないもの（重要！）
- limits.cpu / limits.memory      # これは起動後の制限
```

### Podがスケジュールされない理由
| 原因 | 状態 |
|------|------|
| リソース不足 | Pending (Insufficient cpu/memory) |
| taint不一致 | Pending (node(s) had taints) |
| nodeSelector不一致 | Pending (node(s) didn't match) |
| PVC未バインド | Pending (waiting for PVC) |

---

## 4. kube-controller-manager

### 役割
- 複数のコントローラを単一プロセスで実行
- Desired State と Actual State を一致させる

### 含まれるコントローラ
```
- Node Controller        : ノードの健全性監視
- Replication Controller : レプリカ数の維持
- Endpoints Controller   : Service-Podの紐付け
- ServiceAccount Controller : SA/Token生成
- Namespace Controller   : Namespace削除時のクリーンアップ
```

### 試験で問われる動作
```bash
# Node Controller の動作
- ノードの NotReady 判定
- node-monitor-period: 5s（ハートビート間隔）
- node-monitor-grace-period: 40s（許容時間）
- pod-eviction-timeout: 5m（Pod退避までの時間）

# controller-managerが停止すると
- レプリカ数が自動調整されない
- ノードのNotReady判定が行われない
- ただし既存Podは動き続ける
```

---

## 5. kubelet（ノードコンポーネント）

### 役割
- 各ノード上で動作
- API Serverからの指示でPodを起動
- コンテナランタイムと連携

### 試験ポイント
```bash
# kubeletの設定ファイル
/var/lib/kubelet/config.yaml

# kubeletのサービス
systemctl status kubelet
systemctl restart kubelet

# kubeletが停止すると
- ノードがNotReadyになる
- 新規Podがそのノードで起動しない
- 既存Podは動作を継続（コンテナランタイムが管理）
```

### 静的Pod
```bash
# 静的Podの定義場所
/etc/kubernetes/manifests/

# 静的Podの特徴
- kubeletが直接管理
- API Serverを経由しない
- ノード名がPod名に付与される
- コントロールプレーンのコンポーネントはこれで動作
```

---

## 6. kube-proxy（ノードコンポーネント）

### 役割
- Serviceの通信を実現
- iptables / IPVS ルールを管理

### 試験ポイント
```bash
# kube-proxyの動作モード
- iptables（デフォルト）
- IPVS（大規模クラスタ向け）

# kube-proxyが停止すると
- Service経由の通信ができなくなる
- Pod間の直接通信は可能
- ClusterIP, NodePortが機能しなくなる
```

---

## コンポーネント配置まとめ

| コンポーネント | 配置場所 | 管理方法 |
|---------------|---------|---------|
| kube-apiserver | Master | 静的Pod |
| etcd | Master | 静的Pod |
| kube-scheduler | Master | 静的Pod |
| kube-controller-manager | Master | 静的Pod |
| kubelet | 全ノード | systemd |
| kube-proxy | 全ノード | DaemonSet |

---

## 障害時の影響まとめ（試験頻出）

| 停止コンポーネント | 影響 |
|------------------|------|
| kube-apiserver | kubectl不可、新規作成不可、既存Pod継続 |
| etcd | 全ての書き込み不可 |
| kube-scheduler | 新規Podがスケジュールされない（Pending） |
| kube-controller-manager | レプリカ調整不可、NotReady判定不可 |
| kubelet | ノードNotReady、新規Pod起動不可 |
| kube-proxy | Service通信不可 |

---

## 確認コマンド集

```bash
# コントロールプレーンPodの確認
kubectl get pods -n kube-system

# コンポーネントの健全性
kubectl get componentstatuses  # 非推奨だが試験で出る可能性

# ノード状態
kubectl get nodes
kubectl describe node <node-name>

# 静的Podの確認
ls /etc/kubernetes/manifests/
```

---

## 7. CoreDNS（クラスタDNS）

### 役割
- クラスタ内部のDNS解決
- Service名 → ClusterIP の名前解決
- Pod間通信の名前解決を提供

### 試験ポイント（問題8で使用）
```bash
# CoreDNS Podの確認
kubectl get pods -n kube-system -l k8s-app=kube-dns

# CoreDNS Serviceの確認
kubectl get svc -n kube-system kube-dns

# CoreDNSのログ確認
kubectl logs -n kube-system -l k8s-app=kube-dns
```

### Service FQDN形式
```
<service-name>.<namespace>.svc.cluster.local

例:
- kubernetes.default.svc.cluster.local  # API Server
- kube-dns.kube-system.svc.cluster.local  # CoreDNS自身
- my-svc.my-namespace.svc.cluster.local  # ユーザーService
```

### Pod内のDNS設定（/etc/resolv.conf）
```bash
# Pod内で確認
kubectl exec <pod> -- cat /etc/resolv.conf

# 出力例:
# nameserver 10.96.0.10  ← kube-dns ServiceのClusterIP
# search <ns>.svc.cluster.local svc.cluster.local cluster.local
# options ndots:5
```

### DNS解決テスト
```bash
# Pod内からnslookupで確認
kubectl exec <pod> -- nslookup kubernetes.default.svc.cluster.local

# 短縮名でも解決可能（同一Namespace内）
kubectl exec <pod> -n default -- nslookup kubernetes
```

---

## 8. コントロールプレーンのTaint（問題4で使用）

### デフォルトTaint
```bash
# コントロールプレーンノードのTaint確認
kubectl describe node <control-plane-node> | grep Taint

# 出力例:
# Taints: node-role.kubernetes.io/control-plane:NoSchedule
```

### 意味
- 通常のワークロードはコントロールプレーンにスケジュールされない
- DaemonSet等でコントロールプレーンでも動作させたい場合はTolerationが必要

### Toleration例（DaemonSetで使用）
```yaml
tolerations:
- key: "node-role.kubernetes.io/control-plane"
  operator: "Exists"
  effect: "NoSchedule"
```

---

## ミニ演習（kind対応）

### 演習1: コントロールプレーンコンポーネントの確認

```bash
# 環境準備（kindクラスタ起動済みの前提）
kubectl get pods -n kube-system
```

**やってみよう:**
1. kube-system名前空間のPodを一覧表示
2. 各コンポーネント（apiserver, scheduler, controller-manager, etcd）を特定
3. coredns, kube-proxy も確認

**確認コマンド:**
```bash
# コントロールプレーンのPod
kubectl get pods -n kube-system -l tier=control-plane

# 各コンポーネントの詳細
kubectl describe pod -n kube-system kube-apiserver-cka-control-plane
```

### 演習2: ノードコンポーネントの確認

```bash
# ノード情報
kubectl get nodes -o wide

# ノードの詳細（Conditions, Capacity確認）
kubectl describe node cka-control-plane
```

**確認ポイント:**
- Ready条件がTrueか
- kubeletバージョン
- Container Runtime

### 演習3: コンポーネント停止の影響（理解確認）

以下の質問に答えてください（実行不要）:

1. **kube-apiserver停止時**: kubectlは使える？既存Podは動く？
2. **kube-scheduler停止時**: 新規Podはどうなる？
3. **kubelet停止時**: そのノードのPodはどうなる？

<details>
<summary>解答</summary>

1. kubectlは使えない、既存Podは動き続ける
2. 新規PodはPendingになる（スケジュールされない）
3. 既存Podは動き続けるが、ノードはNotReadyになる

</details>

---

## 段階的ハンズオン（実技問題への橋渡し）

### レベル1: 基礎（コンポーネント確認）

#### 演習1-1: コントロールプレーンコンポーネントの確認

```bash
# kube-systemのPod一覧
kubectl get pods -n kube-system

# control-planeラベルでフィルタ
kubectl get pods -n kube-system -l tier=control-plane

# 各コンポーネントの詳細確認
kubectl describe pod -n kube-system kube-apiserver-cka-control-plane | head -30
```

**確認ポイント:**
- どのコンポーネントが静的Podか
- 各コンポーネントのイメージ名

#### 演習1-2: CoreDNS の確認

```bash
# CoreDNS Podの確認
kubectl get pods -n kube-system -l k8s-app=kube-dns

# CoreDNS Serviceの確認
kubectl get svc -n kube-system kube-dns

# ClusterIPをメモ（後で使う）
COREDNS_IP=$(kubectl get svc -n kube-system kube-dns -o jsonpath='{.spec.clusterIP}')
echo "CoreDNS IP: $COREDNS_IP"
```

**確認ポイント:**
- CoreDNS は Deployment として動作
- kube-dns Service の ClusterIP が Pod の /etc/resolv.conf に設定される

#### 演習1-3: DaemonSet の確認

```bash
# DaemonSetの一覧
kubectl get daemonset -n kube-system

# kube-proxy と kindnet（kind環境）の確認
kubectl describe daemonset kube-proxy -n kube-system | head -40

# 各ノードで1つずつ動作していることを確認
kubectl get pods -n kube-system -o wide | grep kube-proxy
```

**確認ポイント:**
- DaemonSet は全ノードで1つずつPodを実行
- kube-proxy は Service の通信を実現

---

### レベル2: 応用（ログとDNS解決）

#### 演習2-1: コンポーネントログの確認

```bash
# CoreDNS のログ
kubectl logs -n kube-system -l k8s-app=kube-dns --tail=20

# kube-proxy のログ（1つのPod）
kubectl logs -n kube-system -l k8s-app=kube-proxy --tail=20 | head -20

# apiserver のログ（kind環境）
kubectl logs -n kube-system kube-apiserver-cka-control-plane --tail=10
```

**理解ポイント:**
- ログは `-l` でラベル指定可能
- `--tail` で行数制限

#### 演習2-2: DNS解決テスト

```bash
# テスト用Podを作成
kubectl run dns-test --image=busybox --rm -it --restart=Never -- /bin/sh -c '
  echo "=== resolv.conf ==="
  cat /etc/resolv.conf
  echo ""
  echo "=== nslookup kubernetes ==="
  nslookup kubernetes.default.svc.cluster.local
  echo ""
  echo "=== nslookup kube-dns ==="
  nslookup kube-dns.kube-system.svc.cluster.local
'
```

**確認ポイント:**
- nameserver が CoreDNS の ClusterIP と一致するか
- FQDN で名前解決できるか

#### 演習2-3: コントロールプレーンのTaint確認

```bash
# コントロールプレーンノードの特定
CP_NODE=$(kubectl get nodes -l node-role.kubernetes.io/control-plane -o jsonpath='{.items[0].metadata.name}')
echo "Control Plane Node: $CP_NODE"

# Taint確認
kubectl describe node $CP_NODE | grep -A 3 Taints

# workerノードのTaint確認（通常はなし）
kubectl describe node cka-worker | grep -A 3 Taints
```

**理解ポイント:**
- コントロールプレーンには `node-role.kubernetes.io/control-plane:NoSchedule` がある
- このTaintがあるため、通常のPodはコントロールプレーンにスケジュールされない

---

### レベル3: 実技問題準備

#### 演習3-1: 問題8シミュレーション（DNS解決）

```bash
# 環境準備
kubectl create namespace dns-test
kubectl run test-pod --image=nginx -n dns-test
kubectl expose pod test-pod --port=80 --name=test-svc -n dns-test
sleep 5

# === DNS確認用Podを作成 ===
kubectl run dns-client --image=busybox -n dns-test --command -- sleep infinity
sleep 5

# === DNS解決テスト ===
# 1. FQDN での解決
kubectl exec -n dns-test dns-client -- nslookup test-svc.dns-test.svc.cluster.local

# 2. 短縮名での解決（同一Namespace内）
kubectl exec -n dns-test dns-client -- nslookup test-svc

# 3. kubernetes API Server
kubectl exec -n dns-test dns-client -- nslookup kubernetes.default.svc.cluster.local

# 4. resolv.conf と CoreDNS IP の一致確認
echo "=== Pod の resolv.conf ==="
kubectl exec -n dns-test dns-client -- cat /etc/resolv.conf

echo "=== CoreDNS Service IP ==="
kubectl get svc -n kube-system kube-dns -o jsonpath='{.spec.clusterIP}'
echo ""

# クリーンアップ
kubectl delete namespace dns-test
```

#### 演習3-2: 問題4シミュレーション（DaemonSet + control-plane）

```bash
# 環境準備
kubectl create namespace ds-cp-test

# DaemonSet作成（Tolerationなし）
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: no-toleration-ds
  namespace: ds-cp-test
spec:
  selector:
    matchLabels:
      name: test-ds
  template:
    metadata:
      labels:
        name: test-ds
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
EOF

# 確認: control-planeにはPodがない
echo "=== Tolerationなし（control-planeにPodなし）==="
kubectl get pods -n ds-cp-test -o wide

# DaemonSet作成（Tolerationあり）
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: with-toleration-ds
  namespace: ds-cp-test
spec:
  selector:
    matchLabels:
      name: test-ds-tol
  template:
    metadata:
      labels:
        name: test-ds-tol
    spec:
      tolerations:
      - key: "node-role.kubernetes.io/control-plane"
        operator: "Exists"
        effect: "NoSchedule"
      containers:
      - name: nginx
        image: nginx:alpine
EOF

sleep 5

# 確認: 全ノードにPodがある
echo "=== Tolerationあり（全ノードにPod）==="
kubectl get pods -n ds-cp-test -o wide

# クリーンアップ
kubectl delete namespace ds-cp-test
```

#### 演習3-3: コンポーネント障害の理解確認

以下の質問に答えてください（実行不要）:

1. CoreDNS が停止すると何が起きる？
2. kube-proxy が停止すると何が起きる？
3. kube-scheduler が停止しても既存Podが動く理由は？

<details>
<summary>解答</summary>

1. **CoreDNS停止**: Service名での名前解決が失敗。IPアドレス直接指定なら通信可能
2. **kube-proxy停止**: Service経由の通信が失敗。Pod IP直接なら通信可能
3. **scheduler停止後も既存Pod動作**: スケジューラは「配置決定」のみ担当。実際の実行はkubeletが管理するため

</details>

---

## チートシート

### コンポーネント確認

```bash
# コントロールプレーンPod
kubectl get pods -n kube-system -l tier=control-plane

# CoreDNS
kubectl get pods -n kube-system -l k8s-app=kube-dns
kubectl get svc -n kube-system kube-dns

# DaemonSet
kubectl get daemonset -n kube-system

# ノード状態
kubectl get nodes -o wide
kubectl describe node <node-name>
```

### ログ確認

```bash
# コンポーネントログ
kubectl logs -n kube-system <pod-name>
kubectl logs -n kube-system -l <label>

# コンテナログ（crictl）
crictl logs <container-id>
```

### DNS確認

```bash
# CoreDNS IP確認
kubectl get svc -n kube-system kube-dns

# Pod内DNS設定
kubectl exec <pod> -- cat /etc/resolv.conf

# DNS解決テスト
kubectl exec <pod> -- nslookup <service-name>.<namespace>.svc.cluster.local
```

### Taint確認

```bash
# ノードのTaint確認
kubectl describe node <node> | grep -A 3 Taints

# control-plane Taint許容（DaemonSet用）
tolerations:
- key: "node-role.kubernetes.io/control-plane"
  operator: "Exists"
  effect: "NoSchedule"
```

### 静的Pod

```bash
# マニフェストの場所
/etc/kubernetes/manifests/

# 静的Podの一覧（kind環境）
docker exec cka-control-plane ls /etc/kubernetes/manifests/
```

### コンポーネント障害影響まとめ

| コンポーネント | 停止時の影響 | 継続するもの |
|--------------|------------|------------|
| apiserver | kubectl不可、新規作成不可 | 既存Pod |
| scheduler | 新規PodがPending | 既存Pod |
| controller-manager | レプリカ調整不可 | 既存Pod |
| etcd | 全書き込み不可 | 既存Pod（読み取りのみ） |
| kubelet | ノードNotReady | 既存Pod（ランタイム管理） |
| kube-proxy | Service通信不可 | Pod IP直接通信 |
| CoreDNS | 名前解決不可 | IP直接通信 |
