# CKA特化: トラブルシューティング

## 概要

トラブルシューティングはCKA試験の**約30%**を占める最重要領域。
系統的なアプローチを身につけることが合格の鍵。

### 実技問題との対応

| 実技問題 | 使用する知識 |
|---------|------------|
| 問題5: PV/PVC | storageClassName、accessModes の確認と修正 |
| 問題6: ResourceQuota | requests/limits の設定、ReplicaSetイベント確認 |
| 問題7: Service | Endpoints、Selector の確認と修正 |
| 問題10: Events | イベント分析、エラー原因特定、Pod修正 |

---

## トラブルシューティングの基本フロー

```
1. 状況把握 (kubectl get)
    ↓
2. 詳細確認 (kubectl describe)
    ↓
3. ログ確認 (kubectl logs / journalctl)
    ↓
4. 設定確認 (マニフェスト / 設定ファイル)
    ↓
5. 修正 (kubectl edit / systemctl)
```

---

## 1. Pod のトラブルシューティング

### Pod状態と初手コマンド

| Pod Status | 意味 | 初手コマンド |
|-----------|------|-------------|
| Pending | スケジュール待ち | `kubectl describe pod` |
| ContainerCreating | イメージPull中 | `kubectl describe pod` |
| ImagePullBackOff | イメージ取得失敗 | `kubectl describe pod` |
| CrashLoopBackOff | 起動後にクラッシュ | `kubectl logs` |
| Running | 正常動作 | - |
| Error | エラー終了 | `kubectl logs` |
| Completed | 正常終了 | - |

### Pending の原因

```bash
# 確認コマンド
kubectl describe pod <pod-name>

# Events を確認して原因特定
```

| 原因 | Events メッセージ | 対処 |
|------|------------------|------|
| リソース不足 | Insufficient cpu/memory | requests を下げる or ノード追加 |
| nodeSelector不一致 | node(s) didn't match | ラベルを確認 |
| Taint/Toleration | node(s) had taints | Toleration追加 |
| PVC未バインド | persistentvolumeclaim not found | PVCを確認 |

### ImagePullBackOff の原因

```bash
# 確認コマンド
kubectl describe pod <pod-name> | grep -A5 Events

# よくある原因
- イメージ名のタイポ
- タグの間違い
- プライベートレジストリの認証エラー
- ネットワーク問題
```

### CrashLoopBackOff の原因

```bash
# ログ確認（最重要）
kubectl logs <pod-name>

# 前回のログ
kubectl logs <pod-name> --previous

# よくある原因
- コマンドの間違い
- 設定ファイルのエラー
- 依存サービスへの接続失敗
- メモリ不足 (OOMKilled)
```

---

## 2. Node のトラブルシューティング

### Node状態の確認

```bash
# ノード状態
kubectl get nodes

# 詳細確認
kubectl describe node <node-name>
```

### NotReady の原因と対処

```
Node NotReady の主な原因:
1. kubelet が停止している
2. ネットワーク問題
3. ディスク容量不足
4. メモリ不足
```

### kubelet の確認（ノード上で実行）

```bash
# SSH でノードに接続
ssh <node-name>

# kubelet のステータス確認
systemctl status kubelet

# kubelet のログ確認
journalctl -u kubelet -f

# kubelet を再起動
systemctl restart kubelet
```

### ノードの Conditions 確認

```bash
kubectl describe node <node-name> | grep -A10 Conditions

# Conditions の種類
- Ready: kubelet が正常
- MemoryPressure: メモリ不足
- DiskPressure: ディスク不足
- PIDPressure: プロセス過多
- NetworkUnavailable: ネットワーク問題
```

---

## 3. コントロールプレーンのトラブルシューティング

### コントロールプレーンPodの確認

```bash
# kube-system の Pod を確認
kubectl get pods -n kube-system

# 詳細確認
kubectl describe pod <pod-name> -n kube-system

# ログ確認
kubectl logs <pod-name> -n kube-system
```

### 静的Podのトラブルシューティング

```bash
# 静的Podのマニフェストは以下にある
ls /etc/kubernetes/manifests/

# 主要な静的Pod
- kube-apiserver.yaml
- kube-controller-manager.yaml
- kube-scheduler.yaml
- etcd.yaml

# マニフェストを修正すると自動的に再起動される
vi /etc/kubernetes/manifests/kube-apiserver.yaml
```

### APIサーバーが動かない場合

```bash
# kubelet のログを確認（APIサーバーの起動エラー）
journalctl -u kubelet | grep apiserver

# 静的Podのマニフェストを確認
cat /etc/kubernetes/manifests/kube-apiserver.yaml

# よくある原因
- 証明書パスの間違い
- ポート番号の間違い
- etcdのエンドポイントエラー
```

---

## 4. Service のトラブルシューティング

### Serviceの通信確認

```bash
# Service の確認
kubectl get svc <service-name>

# Endpoints の確認（重要！）
kubectl get endpoints <service-name>

# Endpoints が空の場合 → selector が不一致
```

### ServiceとPodの紐付け確認

```bash
# Service の selector を確認
kubectl describe svc <service-name>

# Pod の labels を確認
kubectl get pods --show-labels

# selector と labels が一致しているか確認
```

### よくある問題

| 問題 | 原因 | 対処 |
|-----|------|------|
| Endpoints が空 | selector不一致 | Serviceのselectorを修正 |
| 接続できない | targetPortが間違い | Pod側のポートと一致させる |
| 名前解決できない | CoreDNS問題 | CoreDNS Podを確認 |

---

## 5. DNS のトラブルシューティング

### CoreDNS の確認

```bash
# CoreDNS の Pod 確認
kubectl get pods -n kube-system -l k8s-app=kube-dns

# CoreDNS のログ
kubectl logs -n kube-system -l k8s-app=kube-dns

# CoreDNS の ConfigMap
kubectl get configmap coredns -n kube-system -o yaml
```

### DNS テスト

```bash
# テスト用 Pod を起動
kubectl run test --image=busybox:1.28 --rm -it -- sh

# DNS テスト
nslookup kubernetes.default
nslookup <service-name>.<namespace>.svc.cluster.local
```

---

## 6. ネットワークのトラブルシューティング

### kube-proxy の確認

```bash
# kube-proxy の確認
kubectl get pods -n kube-system | grep kube-proxy

# kube-proxy のログ
kubectl logs -n kube-system -l k8s-app=kube-proxy
```

### Pod間通信テスト

```bash
# テストPod から他のPodへ通信
kubectl exec -it <pod-name> -- curl <target-pod-ip>:<port>

# Service経由でテスト
kubectl exec -it <pod-name> -- curl <service-name>:<port>
```

### NetworkPolicy の確認

```bash
# NetworkPolicy 一覧
kubectl get networkpolicy -A

# 詳細確認
kubectl describe networkpolicy <name> -n <namespace>

# NetworkPolicy が通信をブロックしている可能性
```

---

## 7. ストレージのトラブルシューティング

### PVC が Bound にならない

```bash
# PVC 状態確認
kubectl get pvc

# PV 状態確認
kubectl get pv

# 詳細確認
kubectl describe pvc <pvc-name>
kubectl describe pv <pv-name>
```

### PV-PVC マッチ条件

| 項目 | 条件 |
|-----|------|
| capacity | PV >= PVC |
| accessModes | 一致が必要 |
| storageClassName | 一致が必要（空も含む） |
| volumeMode | 一致が必要 |

---

## 8. 証明書のトラブルシューティング

### 証明書の有効期限確認

```bash
# kubeadm で確認
kubeadm certs check-expiration

# openssl で確認
openssl x509 -in /etc/kubernetes/pki/apiserver.crt -noout -dates
```

### 証明書の更新

```bash
# 全証明書を更新
kubeadm certs renew all

# 個別に更新
kubeadm certs renew apiserver
```

---

## トラブルシューティングチェックリスト

### Pod が動かない
```
□ kubectl get pods で状態確認
□ kubectl describe pod で Events 確認
□ kubectl logs でログ確認
□ イメージ名/タグは正しいか
□ リソース要求は適切か
□ nodeSelector/Toleration は正しいか
```

### Node が NotReady
```
□ ssh でノードに接続
□ systemctl status kubelet
□ journalctl -u kubelet
□ ディスク容量 (df -h)
□ メモリ (free -m)
□ systemctl restart kubelet
```

### Service に接続できない
```
□ kubectl get endpoints で確認
□ selector と labels が一致するか
□ targetPort は正しいか
□ Pod は Running か
□ NetworkPolicy がブロックしていないか
```

### コントロールプレーン障害
```
□ kubectl get pods -n kube-system
□ /etc/kubernetes/manifests/ の確認
□ journalctl -u kubelet
□ 証明書の有効期限
□ etcd の状態
```

---

## 重要コマンド集

```bash
# 状態確認
kubectl get pods -A
kubectl get nodes
kubectl get events --sort-by='.lastTimestamp'

# 詳細確認
kubectl describe pod <pod>
kubectl describe node <node>

# ログ確認
kubectl logs <pod>
kubectl logs <pod> --previous
kubectl logs <pod> -c <container>

# ノード上での確認
systemctl status kubelet
journalctl -u kubelet
journalctl -u kubelet | tail -100

# ネットワーク確認
kubectl get endpoints
kubectl get svc
kubectl exec -it <pod> -- curl <target>

# 設定確認
cat /etc/kubernetes/manifests/*.yaml
cat /var/lib/kubelet/config.yaml
```

---

## 段階的ハンズオン

### レベル1: 基礎（単一コマンドの練習）

#### 演習1-1: 基本的な状態確認

```bash
# Pod状態確認
kubectl get pods -A

# ノード状態確認
kubectl get nodes

# イベント確認（時系列）
kubectl get events --sort-by='.lastTimestamp'

# 特定Namespaceのイベント
kubectl get events -n kube-system

# Pod詳細確認
kubectl describe pod <pod-name>
```

#### 演習1-2: ログ確認の基本

```bash
# Podログ確認
kubectl logs <pod-name>

# 複数コンテナの場合
kubectl logs <pod-name> -c <container-name>

# 前回のログ（クラッシュ後）
kubectl logs <pod-name> --previous

# リアルタイムログ
kubectl logs -f <pod-name>
```

#### 演習1-3: リソース関係の確認

```bash
# PV/PVC確認
kubectl get pv
kubectl get pvc

# Service/Endpoints確認
kubectl get svc
kubectl get endpoints

# ResourceQuota確認
kubectl get resourcequota -A
```

---

### レベル2: 応用（原因特定と修正）

#### 演習2-1: ImagePullBackOff の調査と修正

```bash
# 壊れたPodを作成
kubectl run broken-image --image=nginx:nonexistent-tag-12345

# 状態確認（ImagePullBackOff/ErrImagePull）
kubectl get pod broken-image

# 詳細確認
kubectl describe pod broken-image | grep -A 10 Events

# 修正：Podを削除して正しいイメージで再作成
kubectl delete pod broken-image
kubectl run broken-image --image=nginx:alpine

# 確認
kubectl get pod broken-image
```

#### 演習2-2: Pending Podの調査

```bash
# リソース要求が大きすぎるPodを作成
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: resource-hog
spec:
  containers:
  - name: nginx
    image: nginx
    resources:
      requests:
        memory: "100Gi"
        cpu: "100"
EOF

# 状態確認（Pending）
kubectl get pod resource-hog

# 詳細確認（FailedScheduling）
kubectl describe pod resource-hog | grep -A 5 Events

# クリーンアップ
kubectl delete pod resource-hog
```

#### 演習2-3: Service接続問題の調査

```bash
# バックエンドPodを作成
kubectl run backend --image=nginx --port=80 --labels="app=backend"

# 間違ったSelectorでServiceを作成
kubectl expose pod backend --name=wrong-svc --port=80 --selector="app=wrong-label"

# Endpoints確認（空になっている）
kubectl get endpoints wrong-svc

# 調査
kubectl describe svc wrong-svc | grep Selector
kubectl get pod backend --show-labels

# 修正
kubectl patch svc wrong-svc -p '{"spec":{"selector":{"app":"backend"}}}'

# 確認
kubectl get endpoints wrong-svc

# クリーンアップ
kubectl delete pod backend
kubectl delete svc wrong-svc
```

#### 演習2-4: CrashLoopBackOff の調査

```bash
# 必ず失敗するPodを作成
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: crash-pod
spec:
  containers:
  - name: fail
    image: busybox
    command: ["sh", "-c", "exit 1"]
EOF

# 状態確認（CrashLoopBackOff）
sleep 30
kubectl get pod crash-pod

# ログ確認
kubectl logs crash-pod

# 前回のログ確認
kubectl logs crash-pod --previous

# クリーンアップ
kubectl delete pod crash-pod
```

---

### レベル3: 実技問題準備

#### 演習3-1: 問題5シミュレーション（PV/PVC binding）

```bash
# シナリオ: PVCがPendingのまま → 原因を特定して修正

# 1. PVとPVC（storageClassNameが不一致）を作成
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: PersistentVolume
metadata:
  name: test-pv
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  storageClassName: standard
  hostPath:
    path: /tmp/test-pv
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: test-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: wrong-class
EOF

# 2. 状態確認（PVCがPending）
kubectl get pvc test-pvc
kubectl get pv test-pv

# 3. 原因調査
kubectl describe pvc test-pvc
# → storageClassName が "wrong-class" になっている

kubectl describe pv test-pv
# → storageClassName は "standard"

# 4. 修正：PVCを削除して正しいstorageClassNameで再作成
kubectl delete pvc test-pvc

cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: test-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: standard
EOF

# 5. 確認（Boundになる）
kubectl get pvc test-pvc
kubectl get pv test-pv

# クリーンアップ
kubectl delete pvc test-pvc
kubectl delete pv test-pv
```

#### 演習3-2: 問題6シミュレーション（ResourceQuota）

```bash
# シナリオ: ReplicaSetのPodが起動しない → ResourceQuota原因を特定

# 1. Namespaceとquotaを作成
kubectl create namespace quota-test

cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: quota-test
spec:
  hard:
    requests.cpu: "1"
    requests.memory: "1Gi"
    limits.cpu: "2"
    limits.memory: "2Gi"
EOF

# 2. resources指定なしのDeploymentを作成（失敗する）
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: quota-deploy
  namespace: quota-test
spec:
  replicas: 2
  selector:
    matchLabels:
      app: quota-app
  template:
    metadata:
      labels:
        app: quota-app
    spec:
      containers:
      - name: nginx
        image: nginx
EOF

# 3. 状態確認（Podが作成されない）
kubectl get deployment -n quota-test quota-deploy
kubectl get pods -n quota-test

# 4. 原因調査（ReplicaSetのEvents確認）
kubectl get rs -n quota-test
kubectl describe rs -n quota-test | grep -A 10 Events
# → "Error creating: pods ... must specify requests/limits"

# 5. 修正：Deploymentにresourcesを追加
kubectl patch deployment quota-deploy -n quota-test --type='json' -p='[
  {"op": "add", "path": "/spec/template/spec/containers/0/resources", "value": {
    "requests": {"cpu": "100m", "memory": "128Mi"},
    "limits": {"cpu": "200m", "memory": "256Mi"}
  }}
]'

# 6. 確認
kubectl get pods -n quota-test
# → Podが起動する

# クリーンアップ
kubectl delete namespace quota-test
```

#### 演習3-3: 問題7シミュレーション（Service Selector修正）

```bash
# シナリオ: ServiceのEndpointsが空 → Selector修正

# 1. Deployment作成
kubectl create deployment web --image=nginx --replicas=2

# 2. 間違ったSelectorでService作成
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: web-svc
spec:
  selector:
    app: wrong-app
  ports:
  - port: 80
    targetPort: 80
EOF

# 3. 問題確認（Endpointsが空）
kubectl get endpoints web-svc
# → ENDPOINTS列が <none>

# 4. 原因調査
kubectl describe svc web-svc | grep Selector
# → Selector: app=wrong-app

kubectl get pods --show-labels | grep web
# → app=web

# 5. 修正
kubectl patch svc web-svc -p '{"spec":{"selector":{"app":"web"}}}'

# 6. 確認
kubectl get endpoints web-svc
# → ENDPOINTS列にIPが表示される

# クリーンアップ
kubectl delete deployment web
kubectl delete svc web-svc
```

#### 演習3-4: 問題10シミュレーション（Events分析とPod修正）

```bash
# シナリオ: Eventsからエラー原因を特定してPodを修正

# 1. 問題のあるPodを作成（存在しないConfigMap参照）
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: broken-config-pod
spec:
  containers:
  - name: app
    image: nginx
    envFrom:
    - configMapRef:
        name: nonexistent-config
EOF

# 2. 状態確認
kubectl get pod broken-config-pod
# → CreateContainerConfigError

# 3. Events確認
kubectl get events --field-selector involvedObject.name=broken-config-pod
# または
kubectl describe pod broken-config-pod | grep -A 10 Events
# → "configmap \"nonexistent-config\" not found"

# 4. 修正方法1: ConfigMapを作成
kubectl create configmap nonexistent-config --from-literal=key=value

# 5. Podを再作成（または待機）
kubectl delete pod broken-config-pod
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: broken-config-pod
spec:
  containers:
  - name: app
    image: nginx
    envFrom:
    - configMapRef:
        name: nonexistent-config
EOF

# 6. 確認
kubectl get pod broken-config-pod
# → Running

# クリーンアップ
kubectl delete pod broken-config-pod
kubectl delete configmap nonexistent-config
```

---

## チートシート

### 状態別・初手コマンド

| Pod状態 | 最初に実行するコマンド |
|--------|---------------------|
| Pending | `kubectl describe pod <pod>` |
| ImagePullBackOff | `kubectl describe pod <pod>` |
| CrashLoopBackOff | `kubectl logs <pod> --previous` |
| CreateContainerConfigError | `kubectl describe pod <pod>` |
| Error | `kubectl logs <pod>` |

### よく使う調査コマンド

```bash
# イベント確認（最重要）
kubectl get events --sort-by='.lastTimestamp'
kubectl describe pod <pod> | grep -A 10 Events

# Endpoints確認（Service問題）
kubectl get endpoints <svc>

# PV/PVC確認
kubectl get pv,pvc

# ResourceQuota確認
kubectl get resourcequota -A
kubectl describe resourcequota <name> -n <ns>

# ReplicaSetイベント確認
kubectl describe rs <rs-name> | grep -A 10 Events
```

### トラブルシューティング・パターン

| 症状 | 確認ポイント | 典型的な修正 |
|-----|------------|------------|
| PVC Pending | storageClassName, accessModes | PVC再作成 |
| Pod Pending | Events(リソース/Taint/PVC) | requests調整/Toleration追加 |
| Endpoints空 | Selector vs Pod labels | Selector修正 |
| RS Pod作成失敗 | Events(quota) | resources追加 |
| ConfigMap/Secret参照エラー | describe Events | リソース作成 |

### 修正コマンド

```bash
# Service Selector修正
kubectl patch svc <svc> -p '{"spec":{"selector":{"key":"value"}}}'

# Pod resources追加（Deployment経由）
kubectl set resources deployment <deploy> --requests=cpu=100m,memory=128Mi --limits=cpu=200m,memory=256Mi

# PVC再作成（削除→作成）
kubectl delete pvc <pvc>
kubectl apply -f pvc.yaml
```
